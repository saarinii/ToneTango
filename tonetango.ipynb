{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\nfrom tensorflow.keras.utils import to_categorical\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_ravdess(path=\"/kaggle/input/ravdess-emotional-speech-audio\"):\n    emotion_map = {\n        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n    }\n    data = []\n    for dirname, _, filenames in os.walk(path):\n        for file in filenames:\n            if file.endswith(\".wav\"):\n                emotion = emotion_map[file.split(\"-\")[2]]\n                data.append((os.path.join(dirname, file), emotion))\n    return data\n\ndef load_tess(path=\"/kaggle/input/toronto-emotional-speech-set-tess\"):\n    data = []\n    for dirname, _, filenames in os.walk(path):\n        for file in filenames:\n            if file.endswith(\".wav\"):\n                emotion = file.split(\"_\")[2].replace(\".wav\", \"\").lower()\n                if emotion == 'ps': emotion = 'surprised'\n                data.append((os.path.join(dirname, file), emotion))\n    return data\n\ndef load_crema(path=\"/kaggle/input/cremad\"):\n    emotion_map = {\n        'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful',\n        'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'\n    }\n    data = []\n    for dirname, _, filenames in os.walk(path):\n        for file in filenames:\n            if file.endswith(\".wav\"):\n                emotion = emotion_map[file.split(\"_\")[2]]\n                data.append((os.path.join(dirname, file), emotion))\n    return data\n\ndef load_savee(path=\"/kaggle/input/surrey-audiovisual-expressed-emotion-savee\"):\n    emotion_map = {\n        'a': 'angry', 'd': 'disgust', 'f': 'fearful',\n        'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprised'\n    }\n    data = []\n    for file in os.listdir(path):\n        if file.endswith(\".wav\"):\n            prefix = file[:2] if file[:2] in emotion_map else file[0]\n            emotion = emotion_map.get(prefix, 'unknown')\n            data.append((os.path.join(path, file), emotion))\n    return data","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Combine and Visualize ===\nall_data = load_ravdess() + load_tess() + load_crema() + load_savee()\ndf = pd.DataFrame(all_data, columns=[\"path\", \"emotion\"])\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df, x='emotion', order=df['emotion'].value_counts().index)\nplt.title(\"Emotion Count\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Visualization ===\ndef visualize_audio(file_path):\n    data, sr = librosa.load(file_path)\n    plt.figure(figsize=(14, 4))\n    librosa.display.waveshow(data, sr=sr)\n    plt.title(\"Waveplot\")\n    plt.show()\n\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(14, 4))\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.colorbar()\n    plt.title(\"Spectrogram\")\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_emotions = df['emotion'].unique()\nfor emotion in unique_emotions:\n    sample_path = df[df['emotion'] == emotion]['path'].iloc[0]\n    data, sr = librosa.load(sample_path, duration=3, offset=0.5)\n    plt.figure(figsize=(14, 4))\n    librosa.display.waveshow(data, sr=sr)\n    plt.title(f\"Waveplot - {emotion}\")\n    plt.show()\n\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(14, 4))\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.colorbar()\n    plt.title(f\"Spectrogram - {emotion}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Data Augmentation ===\ndef noise(data):\n    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n    return data + noise_amp * np.random.normal(size=data.shape[0])\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(data, sample_rate):\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result = np.hstack((result, zcr))\n\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft))\n\n    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate)\n    mfcc_mean = np.mean(mfcc.T, axis=0)\n    mfcc_delta = np.mean(librosa.feature.delta(mfcc).T, axis=0)\n    mfcc_delta2 = np.mean(librosa.feature.delta(mfcc, order=2).T, axis=0)\n    result = np.hstack((result, mfcc_mean, mfcc_delta, mfcc_delta2))\n\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms))\n\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel))\n    return result\n\ndef augment_and_extract(path):\n    try:\n        data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n        features = []\n\n        # Original\n        features.append(extract_features(data, sample_rate))\n\n        # Noise added\n        noise_data = data + 0.035 * np.random.normal(0, 1, len(data))\n        features.append(extract_features(noise_data, sample_rate))\n\n        # # Pitch shifted only (avoid broken time_stretch)\n        pitch_data = librosa.effects.pitch_shift(data, sr=sample_rate, n_steps=0.7)\n        features.append(extract_features(pitch_data, sample_rate))\n\n            # Add time stretch\n        stretch_data = librosa.effects.time_stretch(data, rate=1.1)\n        features.append(extract_features(stretch_data, sample_rate))\n\n\n        return features\n    except Exception as e:\n        print(f\"Error processing {path}: {e}\")\n        return []\n\n# === Process Dataset ===\nX, y = [], []\nfor path, emotion in df.itertuples(index=False):\n    feats = augment_and_extract(path)\n    for f in feats:\n        X.append(f)\n        y.append(emotion)\n\nif not X:\n    raise ValueError(\"Feature extraction failed for all files. Check dataset integrity.\")\n\nX = np.array(X)\n\n# Scale features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Encode labels\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n\ny_cat = to_categorical(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n\nX_train = X_train[..., np.newaxis]\nX_test = X_test[..., np.newaxis]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CNN Model ===\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nmodel = Sequential([\n    Conv1D(64, 5, activation='relu', input_shape=(X_train.shape[1], 1)),\n    BatchNormalization(),\n    MaxPooling1D(2),\n    Dropout(0.3),\n\n    Conv1D(128, 5, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(2),\n    Dropout(0.3),\n \n    Conv1D(256, 3, activation='relu'),\n    #BatchNormalization(),\n    MaxPooling1D(2),\n    Dropout(0.3),\n\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dropout(0.4),\n    Dense(y_cat.shape[1], activation='softmax')\n])\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n# Add callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=200,\n    batch_size=32,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stop, reduce_lr]\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import regularizers\n\n# L2 regularization \nl2_lambda = 0.00000001\n\nmodel = Sequential([\n    Conv1D(64, 5, activation='relu', input_shape=(X_train.shape[1], 1),\n           kernel_regularizer=regularizers.l2(l2_lambda)),\n    BatchNormalization(),\n    MaxPooling1D(2),\n    Dropout(0.2),  # Reduced from 0.3\n\n    Conv1D(128, 5, activation='relu',\n           kernel_regularizer=regularizers.l2(l2_lambda)),\n    BatchNormalization(),\n    MaxPooling1D(2),\n    Dropout(0.2),  # Reduced from 0.3\n\n    Conv1D(256, 3, activation='relu',\n           kernel_regularizer=regularizers.l2(l2_lambda)),\n    BatchNormalization(),  # Reintroduced\n    MaxPooling1D(2),\n    Dropout(0.2),  # Reduced from 0.3\n\n    Conv1D(256, 3, activation='relu',  # Added another Conv1D layer\n           kernel_regularizer=regularizers.l2(l2_lambda)),\n    BatchNormalization(),\n    MaxPooling1D(2),\n    Dropout(0.2),\n\n    Flatten(),\n    Dense(512, activation='relu',  # Increased units from 256 to 512\n          kernel_regularizer=regularizers.l2(l2_lambda)),\n    Dropout(0.25),  # Reduced from 0.4\n    Dense(y_cat.shape[1], activation='softmax')\n])\n\n# Compile with AdamW\nmodel.compile(loss='categorical_crossentropy', optimizer=AdamW(learning_rate=0.001), metrics=['accuracy'])\n\n# Callbacks\nearly_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Increased patience\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)  # Lowered min_lr\n\n# Train\nhistory = model.fit(\n    X_train, y_train,\n    epochs=200,\n    batch_size=32,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stop, reduce_lr]\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"emotion_model.h5\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Plot Training History ===\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title(\"Loss Over Epochs\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.title(\"Accuracy Over Epochs\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Evaluation ===\ny_pred = np.argmax(model.predict(X_test), axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nprint(classification_report(y_true, y_pred, target_names=le.classes_))\n\nconf_matrix = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, xticklabels=le.classes_, yticklabels=le.classes_, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport librosa\nfrom IPython.display import Audio\n\n\n# === Load a sample audio and predict ===\nfile_path = \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-01-01-01.wav\"\n# === Load and preprocess audio ===\ndata, sample_rate = librosa.load(file_path, duration=3, offset=0.5)\n\n# === Extract features using the same method used in training ===\nfeature = extract_features(data, sample_rate)\n\n\n# === Scale and reshape ===\nfeature = scaler.transform([feature])  # shape: (1, 202)\nfeature = np.reshape(feature, (1, 202, 1))  # shape: (batch_size, timesteps, 1)\n\n# === Predict ===\nprediction = model.predict(feature)\npredicted_class = np.argmax(prediction)\npredicted_emotion = le.inverse_transform([predicted_class])[0]\n\nprint(f\"Predicted Emotion: {predicted_emotion}\")\nAudio(file_path, rate=sample_rate)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-11T15:23:35.272Z"}},"outputs":[],"execution_count":null}]}